{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210511.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMq8EaHs1w7OAqUKqWvb7+/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yunkoo-GIT/Programming/blob/main/20210511.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1WrDRNLwK_z",
        "outputId": "15d7e88c-6877-49ac-d754-bf371d04fd0f"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "digit=datasets.load_digits()\n",
        "x_train,x_test,y_train,y_test=train_test_split(digit.data,digit.target,train_size=0.6)\n",
        "\n",
        "# MLP 분류기 모델을 학습\n",
        "mlp=MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd',verbose=True)\n",
        "mlp.fit(x_train,y_train)\n",
        "\n",
        "res=mlp.predict(x_test) # 테스트 집합으로 예측\n",
        "\n",
        "# 혼동 행렬\n",
        "conf=np.zeros((10,10))\n",
        "for i in range(len(res)):\n",
        "    conf[res[i]][y_test[i]]+=1\n",
        "print(conf)\n",
        "\n",
        "# 정확률 계산\n",
        "no_correct=0\n",
        "for i in range(10):\n",
        "    no_correct+=conf[i][i]\n",
        "accuracy=no_correct/len(res)\n",
        "print(\"테스트 집합에 대한 정확률은 \", accuracy*100, \"%입니다.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.32135841\n",
            "Iteration 2, loss = 0.30167815\n",
            "Iteration 3, loss = 0.19772501\n",
            "Iteration 4, loss = 0.15853219\n",
            "Iteration 5, loss = 0.12590296\n",
            "Iteration 6, loss = 0.10702985\n",
            "Iteration 7, loss = 0.09177137\n",
            "Iteration 8, loss = 0.08074195\n",
            "Iteration 9, loss = 0.07450855\n",
            "Iteration 10, loss = 0.06651588\n",
            "Iteration 11, loss = 0.06005769\n",
            "Iteration 12, loss = 0.05385727\n",
            "Iteration 13, loss = 0.04972986\n",
            "Iteration 14, loss = 0.04621797\n",
            "Iteration 15, loss = 0.04326540\n",
            "Iteration 16, loss = 0.03882448\n",
            "Iteration 17, loss = 0.03703830\n",
            "Iteration 18, loss = 0.03581385\n",
            "Iteration 19, loss = 0.03322497\n",
            "Iteration 20, loss = 0.03053148\n",
            "Iteration 21, loss = 0.02942642\n",
            "Iteration 22, loss = 0.02737598\n",
            "Iteration 23, loss = 0.02630631\n",
            "Iteration 24, loss = 0.02442121\n",
            "Iteration 25, loss = 0.02335145\n",
            "Iteration 26, loss = 0.02297531\n",
            "Iteration 27, loss = 0.02193550\n",
            "Iteration 28, loss = 0.02208267\n",
            "Iteration 29, loss = 0.02032561\n",
            "Iteration 30, loss = 0.01891819\n",
            "Iteration 31, loss = 0.01876041\n",
            "Iteration 32, loss = 0.01750199\n",
            "Iteration 33, loss = 0.01728688\n",
            "Iteration 34, loss = 0.01672554\n",
            "Iteration 35, loss = 0.01606770\n",
            "Iteration 36, loss = 0.01560565\n",
            "Iteration 37, loss = 0.01516880\n",
            "Iteration 38, loss = 0.01455242\n",
            "Iteration 39, loss = 0.01420221\n",
            "Iteration 40, loss = 0.01406504\n",
            "Iteration 41, loss = 0.01380900\n",
            "Iteration 42, loss = 0.01296401\n",
            "Iteration 43, loss = 0.01264370\n",
            "Iteration 44, loss = 0.01249969\n",
            "Iteration 45, loss = 0.01188384\n",
            "Iteration 46, loss = 0.01212433\n",
            "Iteration 47, loss = 0.01143627\n",
            "Iteration 48, loss = 0.01116110\n",
            "Iteration 49, loss = 0.01102116\n",
            "Iteration 50, loss = 0.01080190\n",
            "Iteration 51, loss = 0.01067404\n",
            "Iteration 52, loss = 0.01017255\n",
            "Iteration 53, loss = 0.01042485\n",
            "Iteration 54, loss = 0.00980357\n",
            "Iteration 55, loss = 0.00970471\n",
            "Iteration 56, loss = 0.00945584\n",
            "Iteration 57, loss = 0.00932641\n",
            "Iteration 58, loss = 0.00907829\n",
            "Iteration 59, loss = 0.00894503\n",
            "Iteration 60, loss = 0.00881911\n",
            "Iteration 61, loss = 0.00868173\n",
            "Iteration 62, loss = 0.00850396\n",
            "Iteration 63, loss = 0.00841169\n",
            "Iteration 64, loss = 0.00819073\n",
            "Iteration 65, loss = 0.00827862\n",
            "Iteration 66, loss = 0.00790653\n",
            "Iteration 67, loss = 0.00780084\n",
            "Iteration 68, loss = 0.00763591\n",
            "Iteration 69, loss = 0.00754081\n",
            "Iteration 70, loss = 0.00746500\n",
            "Iteration 71, loss = 0.00750263\n",
            "Iteration 72, loss = 0.00723579\n",
            "Iteration 73, loss = 0.00705874\n",
            "Iteration 74, loss = 0.00695411\n",
            "Iteration 75, loss = 0.00684736\n",
            "Iteration 76, loss = 0.00682088\n",
            "Iteration 77, loss = 0.00670669\n",
            "Iteration 78, loss = 0.00659205\n",
            "Iteration 79, loss = 0.00654281\n",
            "Iteration 80, loss = 0.00651091\n",
            "Iteration 81, loss = 0.00630241\n",
            "Iteration 82, loss = 0.00621536\n",
            "Iteration 83, loss = 0.00611942\n",
            "Iteration 84, loss = 0.00603790\n",
            "Iteration 85, loss = 0.00600044\n",
            "Iteration 86, loss = 0.00593744\n",
            "Iteration 87, loss = 0.00582790\n",
            "Iteration 88, loss = 0.00575532\n",
            "Iteration 89, loss = 0.00564023\n",
            "Iteration 90, loss = 0.00559680\n",
            "Iteration 91, loss = 0.00561153\n",
            "Iteration 92, loss = 0.00548912\n",
            "Iteration 93, loss = 0.00541124\n",
            "Iteration 94, loss = 0.00538507\n",
            "Iteration 95, loss = 0.00529507\n",
            "Iteration 96, loss = 0.00521828\n",
            "Iteration 97, loss = 0.00513239\n",
            "Iteration 98, loss = 0.00510150\n",
            "Iteration 99, loss = 0.00505364\n",
            "Iteration 100, loss = 0.00495924\n",
            "Iteration 101, loss = 0.00494145\n",
            "Iteration 102, loss = 0.00487957\n",
            "Iteration 103, loss = 0.00480561\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[71.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0. 73.  1.  0.  0.  0.  0.  0.  2.  0.]\n",
            " [ 0.  0. 70.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. 65.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. 72.  0.  0.  1.  2.  0.]\n",
            " [ 0.  0.  0.  2.  0. 70.  0.  0.  1.  0.]\n",
            " [ 0.  1.  0.  0.  0.  0. 66.  0.  0.  0.]\n",
            " [ 0.  0.  1.  0.  0.  0.  0. 74.  1.  0.]\n",
            " [ 0.  0.  0.  1.  0.  0.  1.  1. 62.  3.]\n",
            " [ 0.  0.  0.  0.  2.  4.  0.  0.  0. 72.]]\n",
            "테스트 집합에 대한 정확률은  96.66203059805285 %입니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR5OeXe3wU3j",
        "outputId": "fee17ac4-39ba-4c6c-f37a-d236c7b74df1"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# MNIST 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "mnist=fetch_openml('mnist_784')\n",
        "mnist.data=mnist.data/255.0\n",
        "x_train=mnist.data[:60000]; x_test=mnist.data[60000:]\n",
        "y_train=np.int16(mnist.target[:60000]); y_test=np.int16(mnist.target[60000:])\n",
        "\n",
        "# MLP 분류기 모델을 학습\n",
        "mlp=MLPClassifier(hidden_layer_sizes=(50),learning_rate_init=0.001,batch_size=128,max_iter=300,solver='adam',verbose=True)\n",
        "mlp.fit(x_train,y_train)\n",
        "\n",
        "# 테스트 집합으로 예측\n",
        "res=mlp.predict(x_test)\n",
        "\n",
        "# 혼동 행렬\n",
        "conf=np.zeros((10,10),dtype=np.int16)\n",
        "for i in range(len(res)):\n",
        "    conf[res[i]][y_test[i]]+=1\n",
        "print(conf)\n",
        "\n",
        "# 정확률 계산\n",
        "no_correct=0\n",
        "for i in range(10):\n",
        "    no_correct+=conf[i][i]\n",
        "accuracy=no_correct/len(res)\n",
        "print(\"테스트 집합에 대한 정확률은\", accuracy*100, \"%입니다.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.45632695\n",
            "Iteration 2, loss = 0.21505453\n",
            "Iteration 3, loss = 0.16647319\n",
            "Iteration 4, loss = 0.13785501\n",
            "Iteration 5, loss = 0.11995445\n",
            "Iteration 6, loss = 0.10548224\n",
            "Iteration 7, loss = 0.09458827\n",
            "Iteration 8, loss = 0.08545334\n",
            "Iteration 9, loss = 0.07696550\n",
            "Iteration 10, loss = 0.07093679\n",
            "Iteration 11, loss = 0.06551156\n",
            "Iteration 12, loss = 0.05921903\n",
            "Iteration 13, loss = 0.05554850\n",
            "Iteration 14, loss = 0.05044954\n",
            "Iteration 15, loss = 0.04755221\n",
            "Iteration 16, loss = 0.04377534\n",
            "Iteration 17, loss = 0.04012834\n",
            "Iteration 18, loss = 0.03743686\n",
            "Iteration 19, loss = 0.03562109\n",
            "Iteration 20, loss = 0.03267076\n",
            "Iteration 21, loss = 0.03029703\n",
            "Iteration 22, loss = 0.02853161\n",
            "Iteration 23, loss = 0.02679584\n",
            "Iteration 24, loss = 0.02442911\n",
            "Iteration 25, loss = 0.02331952\n",
            "Iteration 26, loss = 0.02132630\n",
            "Iteration 27, loss = 0.02019974\n",
            "Iteration 28, loss = 0.01858840\n",
            "Iteration 29, loss = 0.01769283\n",
            "Iteration 30, loss = 0.01626852\n",
            "Iteration 31, loss = 0.01634570\n",
            "Iteration 32, loss = 0.01417130\n",
            "Iteration 33, loss = 0.01317764\n",
            "Iteration 34, loss = 0.01225526\n",
            "Iteration 35, loss = 0.01190597\n",
            "Iteration 36, loss = 0.01089495\n",
            "Iteration 37, loss = 0.01012309\n",
            "Iteration 38, loss = 0.00948732\n",
            "Iteration 39, loss = 0.00919176\n",
            "Iteration 40, loss = 0.00943540\n",
            "Iteration 41, loss = 0.00737780\n",
            "Iteration 42, loss = 0.00749291\n",
            "Iteration 43, loss = 0.00712852\n",
            "Iteration 44, loss = 0.00702298\n",
            "Iteration 45, loss = 0.00627893\n",
            "Iteration 46, loss = 0.00632942\n",
            "Iteration 47, loss = 0.00642952\n",
            "Iteration 48, loss = 0.00622651\n",
            "Iteration 49, loss = 0.00529153\n",
            "Iteration 50, loss = 0.00465022\n",
            "Iteration 51, loss = 0.00494670\n",
            "Iteration 52, loss = 0.00493049\n",
            "Iteration 53, loss = 0.00561782\n",
            "Iteration 54, loss = 0.00412210\n",
            "Iteration 55, loss = 0.00333145\n",
            "Iteration 56, loss = 0.00391825\n",
            "Iteration 57, loss = 0.00597247\n",
            "Iteration 58, loss = 0.00405780\n",
            "Iteration 59, loss = 0.00257083\n",
            "Iteration 60, loss = 0.00285420\n",
            "Iteration 61, loss = 0.00265699\n",
            "Iteration 62, loss = 0.00766034\n",
            "Iteration 63, loss = 0.00356535\n",
            "Iteration 64, loss = 0.00228213\n",
            "Iteration 65, loss = 0.00206490\n",
            "Iteration 66, loss = 0.00221390\n",
            "Iteration 67, loss = 0.00236689\n",
            "Iteration 68, loss = 0.01052609\n",
            "Iteration 69, loss = 0.00252264\n",
            "Iteration 70, loss = 0.00206344\n",
            "Iteration 71, loss = 0.00184211\n",
            "Iteration 72, loss = 0.00174923\n",
            "Iteration 73, loss = 0.00173428\n",
            "Iteration 74, loss = 0.00978241\n",
            "Iteration 75, loss = 0.00438974\n",
            "Iteration 76, loss = 0.00199841\n",
            "Iteration 77, loss = 0.00173609\n",
            "Iteration 78, loss = 0.00166612\n",
            "Iteration 79, loss = 0.00164047\n",
            "Iteration 80, loss = 0.00160755\n",
            "Iteration 81, loss = 0.00875612\n",
            "Iteration 82, loss = 0.00583300\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[ 961    0    2    2    1    2    5    1    1    2]\n",
            " [   1 1123    5    0    0    1    2    8    1    2]\n",
            " [   4    4 1000   11    4    0    1   14    5    1]\n",
            " [   1    0    1  977    1   15    1    3    6    5]\n",
            " [   1    0    1    0  962    2    6    2    6   19]\n",
            " [   0    1    1    5    0  858    7    2    6    4]\n",
            " [   3    1    3    0    3    6  932    0    3    0]\n",
            " [   2    1    6    7    2    1    0  990    5    4]\n",
            " [   4    5   12    6    1    3    4    2  937    1]\n",
            " [   3    0    1    2    8    4    0    6    4  971]]\n",
            "테스트 집합에 대한 정확률은 97.11 %입니다.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}