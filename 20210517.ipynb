{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210517.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOhhyzzZkKA4aM6957En8bj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yunkoo-GIT/Programming/blob/main/20210517.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIgl9f2tVXIv",
        "outputId": "abe70031-8f85-4901-ef45-27826b26bdf8"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "digit=datasets.load_digits()\n",
        "x_train,x_test,y_train,y_test=train_test_split(digit.data,digit.target,train_size=0.6)\n",
        "\n",
        "# MLP 분류기 모델을 학습\n",
        "mlp=MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd',verbose=True)\n",
        "mlp.fit(x_train,y_train)\n",
        "\n",
        "res=mlp.predict(x_test) # 테스트 집합으로 예측\n",
        "\n",
        "# 혼동 행렬\n",
        "conf=np.zeros((10,10))\n",
        "for i in range(len(res)):\n",
        "    conf[res[i]][y_test[i]]+=1\n",
        "print(conf)\n",
        "\n",
        "# 정확률 계산\n",
        "no_correct=0\n",
        "for i in range(10):\n",
        "    no_correct+=conf[i][i]\n",
        "accuracy=no_correct/len(res)\n",
        "print(\"테스트 집합에 대한 정확률은 \", accuracy*100, \"%입니다.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.61516451\n",
            "Iteration 2, loss = 0.38984656\n",
            "Iteration 3, loss = 0.24407871\n",
            "Iteration 4, loss = 0.17551499\n",
            "Iteration 5, loss = 0.14011086\n",
            "Iteration 6, loss = 0.11875492\n",
            "Iteration 7, loss = 0.10396340\n",
            "Iteration 8, loss = 0.09280849\n",
            "Iteration 9, loss = 0.08051496\n",
            "Iteration 10, loss = 0.06995237\n",
            "Iteration 11, loss = 0.06419544\n",
            "Iteration 12, loss = 0.05821743\n",
            "Iteration 13, loss = 0.05565580\n",
            "Iteration 14, loss = 0.05152977\n",
            "Iteration 15, loss = 0.04638284\n",
            "Iteration 16, loss = 0.04244029\n",
            "Iteration 17, loss = 0.04073154\n",
            "Iteration 18, loss = 0.03706397\n",
            "Iteration 19, loss = 0.03640862\n",
            "Iteration 20, loss = 0.03404365\n",
            "Iteration 21, loss = 0.03158064\n",
            "Iteration 22, loss = 0.03012004\n",
            "Iteration 23, loss = 0.02971098\n",
            "Iteration 24, loss = 0.02766411\n",
            "Iteration 25, loss = 0.02601888\n",
            "Iteration 26, loss = 0.02513411\n",
            "Iteration 27, loss = 0.02371632\n",
            "Iteration 28, loss = 0.02304060\n",
            "Iteration 29, loss = 0.02205440\n",
            "Iteration 30, loss = 0.02164351\n",
            "Iteration 31, loss = 0.02079751\n",
            "Iteration 32, loss = 0.02033683\n",
            "Iteration 33, loss = 0.01904934\n",
            "Iteration 34, loss = 0.01849793\n",
            "Iteration 35, loss = 0.01799037\n",
            "Iteration 36, loss = 0.01789975\n",
            "Iteration 37, loss = 0.01738679\n",
            "Iteration 38, loss = 0.01634852\n",
            "Iteration 39, loss = 0.01593001\n",
            "Iteration 40, loss = 0.01581191\n",
            "Iteration 41, loss = 0.01491109\n",
            "Iteration 42, loss = 0.01489834\n",
            "Iteration 43, loss = 0.01446460\n",
            "Iteration 44, loss = 0.01394364\n",
            "Iteration 45, loss = 0.01354614\n",
            "Iteration 46, loss = 0.01327644\n",
            "Iteration 47, loss = 0.01295405\n",
            "Iteration 48, loss = 0.01240609\n",
            "Iteration 49, loss = 0.01319863\n",
            "Iteration 50, loss = 0.01228533\n",
            "Iteration 51, loss = 0.01165295\n",
            "Iteration 52, loss = 0.01150191\n",
            "Iteration 53, loss = 0.01147896\n",
            "Iteration 54, loss = 0.01099962\n",
            "Iteration 55, loss = 0.01073108\n",
            "Iteration 56, loss = 0.01052675\n",
            "Iteration 57, loss = 0.01036618\n",
            "Iteration 58, loss = 0.01010101\n",
            "Iteration 59, loss = 0.00969679\n",
            "Iteration 60, loss = 0.00986051\n",
            "Iteration 61, loss = 0.00953717\n",
            "Iteration 62, loss = 0.00942420\n",
            "Iteration 63, loss = 0.00929654\n",
            "Iteration 64, loss = 0.00905138\n",
            "Iteration 65, loss = 0.00889410\n",
            "Iteration 66, loss = 0.00873578\n",
            "Iteration 67, loss = 0.00867647\n",
            "Iteration 68, loss = 0.00855039\n",
            "Iteration 69, loss = 0.00833157\n",
            "Iteration 70, loss = 0.00823885\n",
            "Iteration 71, loss = 0.00813366\n",
            "Iteration 72, loss = 0.00811610\n",
            "Iteration 73, loss = 0.00792847\n",
            "Iteration 74, loss = 0.00782990\n",
            "Iteration 75, loss = 0.00754403\n",
            "Iteration 76, loss = 0.00750053\n",
            "Iteration 77, loss = 0.00740341\n",
            "Iteration 78, loss = 0.00730200\n",
            "Iteration 79, loss = 0.00722010\n",
            "Iteration 80, loss = 0.00722387\n",
            "Iteration 81, loss = 0.00693729\n",
            "Iteration 82, loss = 0.00693283\n",
            "Iteration 83, loss = 0.00676679\n",
            "Iteration 84, loss = 0.00672729\n",
            "Iteration 85, loss = 0.00660863\n",
            "Iteration 86, loss = 0.00649955\n",
            "Iteration 87, loss = 0.00654676\n",
            "Iteration 88, loss = 0.00637062\n",
            "Iteration 89, loss = 0.00633869\n",
            "Iteration 90, loss = 0.00626759\n",
            "Iteration 91, loss = 0.00620432\n",
            "Iteration 92, loss = 0.00619227\n",
            "Iteration 93, loss = 0.00596145\n",
            "Iteration 94, loss = 0.00593960\n",
            "Iteration 95, loss = 0.00580375\n",
            "Iteration 96, loss = 0.00573197\n",
            "Iteration 97, loss = 0.00574835\n",
            "Iteration 98, loss = 0.00567090\n",
            "Iteration 99, loss = 0.00557635\n",
            "Iteration 100, loss = 0.00551044\n",
            "Iteration 101, loss = 0.00544116\n",
            "Iteration 102, loss = 0.00538910\n",
            "Iteration 103, loss = 0.00533830\n",
            "Iteration 104, loss = 0.00528488\n",
            "Iteration 105, loss = 0.00520067\n",
            "Iteration 106, loss = 0.00517417\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[80.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
            " [ 0. 80.  2.  0.  0.  0.  1.  0.  2.  0.]\n",
            " [ 0.  0. 69.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. 65.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. 74.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0. 69.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0. 66.  0.  1.  0.]\n",
            " [ 0.  0.  1.  0.  1.  0.  0. 71.  0.  0.]\n",
            " [ 0.  1.  0.  0.  3.  0.  1.  0. 64.  1.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  1.  0. 64.]]\n",
            "테스트 집합에 대한 정확률은  97.63560500695411 %입니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRalIZpsVmra",
        "outputId": "a8bedab1-5f86-436b-9f52-b9f127210492"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# MNIST 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "mnist=fetch_openml('mnist_784')\n",
        "mnist.data=mnist.data/255.0\n",
        "x_train=mnist.data[:60000]; x_test=mnist.data[60000:]\n",
        "y_train=np.int16(mnist.target[:60000]); y_test=np.int16(mnist.target[60000:])\n",
        "\n",
        "# MLP 분류기 모델을 학습\n",
        "mlp=MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=512,max_iter=300,solver='adam',verbose=True)\n",
        "mlp.fit(x_train,y_train)\n",
        "\n",
        "# 테스트 집합으로 예측\n",
        "res=mlp.predict(x_test)\n",
        "\n",
        "# 혼동 행렬\n",
        "conf=np.zeros((10,10),dtype=np.int16)\n",
        "for i in range(len(res)):\n",
        "    conf[res[i]][y_test[i]]+=1\n",
        "print(conf)\n",
        "\n",
        "# 정확률 계산\n",
        "no_correct=0\n",
        "for i in range(10):\n",
        "    no_correct+=conf[i][i]\n",
        "accuracy=no_correct/len(res)\n",
        "print(\"테스트 집합에 대한 정확률은\", accuracy*100, \"%입니다.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.60209958\n",
            "Iteration 2, loss = 0.26241138\n",
            "Iteration 3, loss = 0.20775430\n",
            "Iteration 4, loss = 0.17394698\n",
            "Iteration 5, loss = 0.14990843\n",
            "Iteration 6, loss = 0.13210165\n",
            "Iteration 7, loss = 0.11782239\n",
            "Iteration 8, loss = 0.10601204\n",
            "Iteration 9, loss = 0.09567657\n",
            "Iteration 10, loss = 0.08627800\n",
            "Iteration 11, loss = 0.07962798\n",
            "Iteration 12, loss = 0.07294346\n",
            "Iteration 13, loss = 0.06734881\n",
            "Iteration 14, loss = 0.06177631\n",
            "Iteration 15, loss = 0.05672366\n",
            "Iteration 16, loss = 0.05265173\n",
            "Iteration 17, loss = 0.04828030\n",
            "Iteration 18, loss = 0.04561074\n",
            "Iteration 19, loss = 0.04193207\n",
            "Iteration 20, loss = 0.03886684\n",
            "Iteration 21, loss = 0.03658595\n",
            "Iteration 22, loss = 0.03346752\n",
            "Iteration 23, loss = 0.03163645\n",
            "Iteration 24, loss = 0.02977517\n",
            "Iteration 25, loss = 0.02704844\n",
            "Iteration 26, loss = 0.02588732\n",
            "Iteration 27, loss = 0.02372806\n",
            "Iteration 28, loss = 0.02195455\n",
            "Iteration 29, loss = 0.02077368\n",
            "Iteration 30, loss = 0.01901152\n",
            "Iteration 31, loss = 0.01781472\n",
            "Iteration 32, loss = 0.01697095\n",
            "Iteration 33, loss = 0.01559862\n",
            "Iteration 34, loss = 0.01457951\n",
            "Iteration 35, loss = 0.01325015\n",
            "Iteration 36, loss = 0.01257318\n",
            "Iteration 37, loss = 0.01178189\n",
            "Iteration 38, loss = 0.01074794\n",
            "Iteration 39, loss = 0.01040701\n",
            "Iteration 40, loss = 0.00956660\n",
            "Iteration 41, loss = 0.00880043\n",
            "Iteration 42, loss = 0.00818529\n",
            "Iteration 43, loss = 0.00766955\n",
            "Iteration 44, loss = 0.00702098\n",
            "Iteration 45, loss = 0.00700935\n",
            "Iteration 46, loss = 0.00607223\n",
            "Iteration 47, loss = 0.00559652\n",
            "Iteration 48, loss = 0.00532977\n",
            "Iteration 49, loss = 0.00501661\n",
            "Iteration 50, loss = 0.00483686\n",
            "Iteration 51, loss = 0.00452751\n",
            "Iteration 52, loss = 0.00412044\n",
            "Iteration 53, loss = 0.00375500\n",
            "Iteration 54, loss = 0.00363703\n",
            "Iteration 55, loss = 0.00329856\n",
            "Iteration 56, loss = 0.00306046\n",
            "Iteration 57, loss = 0.00291108\n",
            "Iteration 58, loss = 0.00268124\n",
            "Iteration 59, loss = 0.00250722\n",
            "Iteration 60, loss = 0.00357777\n",
            "Iteration 61, loss = 0.00240865\n",
            "Iteration 62, loss = 0.00209441\n",
            "Iteration 63, loss = 0.00192785\n",
            "Iteration 64, loss = 0.00189163\n",
            "Iteration 65, loss = 0.00180861\n",
            "Iteration 66, loss = 0.00178394\n",
            "Iteration 67, loss = 0.00159326\n",
            "Iteration 68, loss = 0.00148300\n",
            "Iteration 69, loss = 0.00141321\n",
            "Iteration 70, loss = 0.00135227\n",
            "Iteration 71, loss = 0.00130207\n",
            "Iteration 72, loss = 0.00121502\n",
            "Iteration 73, loss = 0.00117669\n",
            "Iteration 74, loss = 0.00117632\n",
            "Iteration 75, loss = 0.00107584\n",
            "Iteration 76, loss = 0.00098793\n",
            "Iteration 77, loss = 0.00098525\n",
            "Iteration 78, loss = 0.00095411\n",
            "Iteration 79, loss = 0.00088783\n",
            "Iteration 80, loss = 0.00082434\n",
            "Iteration 81, loss = 0.00081818\n",
            "Iteration 82, loss = 0.00082879\n",
            "Iteration 83, loss = 0.00685324\n",
            "Iteration 84, loss = 0.00832070\n",
            "Iteration 85, loss = 0.00179248\n",
            "Iteration 86, loss = 0.00092361\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[ 969    0    3    1    1    2    4    1    5    1]\n",
            " [   0 1123    2    0    0    0    2    5    0    2]\n",
            " [   1    4 1009    3    3    0    4    7    3    0]\n",
            " [   1    0    4  995    1    7    0    4    8    5]\n",
            " [   0    0    1    0  954    1    2    1    5    7]\n",
            " [   4    1    1    2    1  867    6    0    5    2]\n",
            " [   2    2    2    2    5    6  939    0    2    0]\n",
            " [   1    1    4    2    2    3    1 1001    4    4]\n",
            " [   1    4    5    2    0    4    0    4  937    3]\n",
            " [   1    0    1    3   15    2    0    5    5  985]]\n",
            "테스트 집합에 대한 정확률은 97.78999999999999 %입니다.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}